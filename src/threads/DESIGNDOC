			--------------------
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Evelyn Gillie <egillie@stanford.edu>
Peter Pham <ptpham@stanford.edu>
Diego Pontoriero <dpontori@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct timer_sleeping_thread 
{
  int64_t wakeup_time;
  struct thread *t;
  struct list_elem elem;
};

Struct representing a sleeping thread.  wakeup_time is the earliest tick
that this thread can wake up.

static struct list timer_sleeping_threads;
List of current sleeping threads, managed by the timer.  This list is
ordered by wakeup times, with the soonest wake-up times at the front of
the list.


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In timer_sleep(), we create a timer_sleeping_thread struct for the current
thread, and calculate the earliest wake up time (in ticks).  We then add
this to the global list of sleeping threads, and block the thread.


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The global list of sleeping threads is ordered, with threads with the
soonest wake-up times at the front.  The time interrupt handler only has
to look at the head of the list, check to see if the wake-up time for that
thread has passed, and if it has: wake up that thread, and traverse the
list until we see a thread whose wakeup time has not passed.  At most, the
interrupt handler examines one thread which does not need to be woken,
making the runtime O(number of threads that need to be woken + 1).

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Wherever we traverse, insert into, or delete from our data structures
listed in A1, we disable interrupts, so reads/writes are atomic and
uncorrupted.


>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Interrupts are disabled in the critical sections of timer_sleep, so no
races can occur.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We first considered storing the wait times in the sleeping thread structs,
but we would have needed to decrement the remaining wait time at every
timer interrupt, an O(n) traversal over timer_sleeping_threads.  Our
implementation instead stores the earliest possible wakeup time, which is
absolute and does not need updates.



			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


In thread.h:

  int effective_priority;
	The effective priority of a thread, which takes into priorities
	donated by other threads.

  struct list priority_holding; 
	A list of locks that this threads holds.

  struct lock *priority_waiting;
	A lock, if any, that this thread is waiting on

In sync.h:
  struct list_elem priority_holder; 
	A list element for a thread's priority_holding list

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

		T1		T2	T3		T4
		\		/	\		/
		 \	   /	 \	   /
		  \	  /		  \   /			- T1 and T2 are waiting on L1
			L1			L2			- T2 and T3 are waiting on L2
			|			|			
			T5			T6			-L1 is held by T5, L2 by T6
			\			/
			 \		   /
			  \		  /
			   \	 /
				  L3				- T5 and T6 are waiting on L3
				  |
				  T7				- L3 is held by T7

Starting at the top: T1 and T2 store L1 in their "priority_waiting" field
(and T3, T4 store L2).	L1 stores T1, T2 in its waiters list, and T5 in
its holder field. 



---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

	Whenever we wake up a thread on a waiters list, we search for the
	thread with the highest effective priority.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?
	
	In lock_acquire, if a new thread starts waiting on a lock, we check
	whether its priority is higher than the effective priority of that
	lock's holder.L1's holder, T5.  If it is, we call
	set_effective_priority(lock's holder, priority of current thread);
	set_effective_priority (in addition to setting the effective priority)
	checks to see if the lock's holder is waiting on any locks, and will
	recursively call set_effective_priority on that lock's holder if its
	priority should be propogated.
	

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

	We remove the lock from the list of locks held by the current thread,
	and recalculate the effective priority.  The effective priority
	calculation takes the max of the thread's original priority
	(non-donated), and any priority donations from locks it holds
	(excluding the lock we just removed from its holdings list).

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

When we set a thread's priority, we need to compare it to any potential
donated priorities and compute the max of these priorities.  If, after we
compute the max priority, another thread donates its priority to us, the
just-computed maximum is now stale, and we incorrectly set our priority to
this stale value.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We first planned on keeping the lists of waiting threads and the ready
queue ordered by priority, but decided on keeping the lists unordered and
selecting the thread with the highest priority at retrieval time.  The
bookkeeping involved with keeping the lists ordered unnecessarily
complicated our design, because we would have needed to resort every time
we updated a thread's priority, even indirectly through priority
donations, which might be nested.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

The following global variables were declared:

  struct list mlfqs_lists[PRI_MAX + 1];
This global array is the MLFQS equivalent of the ready_list. There is one
list for each possible priority.

  int mlfqs_num_ready;
This global variable maintains the total number of threads in the mlfqs_lists
array. 

  fp_t load_avg;
This global variable maintains the load_avg as described in the assignment
description.

The following fields were added to the thread struct.
  /* 4.4BSD scheduling data */
  int nice;
  int mlfqs_priority;
  fp_t recent_cpu;
  struct list_elem mlfqs_elem;

The nice field stores the current nice value of the thread.

The mlfqs_priority field stores the current mlfqs priority based on recent_cpu
and the global load_avg as described in the assignment description.

The recent_cpu field stores a value that approximates how much the thread
has been using the CPU recently. 

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu		priority		  thread
ticks   A   B   C   A		B		C	  to run
-----  --  --  --	--		--		--	  ------
0		0	0	0	63.00	61.00	59.00	A
4		4	0	0	62.00	61.00	59.00	A
8		8	0	0	61.00	61.00	59.00	A
12		12	0	0	60.00	61.00	59.00	B
16		12	4	0	60.00	60.00	59.00	B
20		12	8	0	60.00	59.00	59.00	A
24		16	8	0	59.00	59.00	59.00	A
28		20	8	0	58.00	59.00	59.00	C
32		20	8	4	58.00	59.00	58.00	B
36		20	12	4	58.00	58.00	58.00	B

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

If a thread's priority is changed to a priority equal to the current
thread's priority, there was ambiguity about whether we should switch to
the other thread.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Though the design doc specifies updating all threads' priorities every
four ticks, we only update the current thread's priority, since the
factors that influence the priorities of other threads cannot change until
the load average changes.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Our design uses an array of ready lists for each possible priority. Since the 
library list implementation is a doubly linked, this means that insertion
into and removal from a ready list are constant time operations. We maintain
a count of the number of threads in the MLFQS so that computing the load 
average does not require a linear time operation. The difficulty with this is
that we must remember to update this value every time a thread changes to or
from the THREAD_READY state. 

We implemented the MLFQS system in parallel with the original priority
scheduling system. We did this because we wanted to reduce the likelihood of
introducing bugs into the first scheduler. We switch between the two systems at
only a few key points in the code that relate to scheduling: thread_unblock,
thread_yield, next_thread_to_run. Unfortunately we could not separate the two
systems entirely. In order to prevent priority donation from happening under
MLFQS, we do not update effective priorities when thread_mlfqs is set.  This is
implemented simply as a return case in a single function
(thread_set_effective_priority).  On the other hand, we actually want the
influence of yielding based on priorities in the case of locks. Because of
this, our MLFQS system keeps a thread's effective_priority and its
mlfqs_priority the same.

Given additional time we might maintain a variable that stores the highest
priority of any thread at any given moment. This would allow us to find the
next thread during scheduling slightly faster. As it stands now, finding the
next thread involves checking all of the ready lists in decreasing order of
priority until a thread is found. 

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

We implemented fixed point in the simplest way possible. We back the fixed 
point value with a signed integer using a typedef and center the range at 0
(i.e. no bias).  By using a signed integer, we get signedness for free and
multiplication and division work as described in the assignment description.
By using a typedef, we can replace the fixed point representation in the
future if desired. We expose fixed point operations as a set of inlined
functions. We did not use macros because they can introduce subtle bugs. For
example, the order of operations may not be as intended when nesting macros
because of their simple text replacement behavior.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
